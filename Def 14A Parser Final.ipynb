{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapper\n",
    "\n",
    "### 1. Scrapper\n",
    "\n",
    "Only change:\n",
    "1. file_url: year and quarter\n",
    "2. csv file name : year_Q\n",
    "\n",
    "Note: work on putting the scrapper in a function that takes year and quarter as parameters\n",
    "\n",
    "CREDITS TO SIGMA CODING (https://github.com/areed1192/sigma_coding_youtube) FOR THE SCRAPPER PART OF THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Import libraries\n",
    "# import requests\n",
    "# import urllib\n",
    "# from bs4 import BeautifulSoup\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Define a master file url\n",
    "# file_url = r\"https://www.sec.gov/Archives/edgar/full-index/2005/QTR3/master.idx\" # Full index\n",
    "# #file_url = r\"https://www.sec.gov/Archives/edgar/daily-index/2006/QTR2/master.20060627.idx\" # Daily Index\n",
    "\n",
    "# # make a request for the file above\n",
    "# content = requests.get(file_url).content\n",
    "\n",
    "# # write content to a text file\n",
    "# with open('master_20190401.txt', 'wb') as f:\n",
    "#     f.write(content)\n",
    "\n",
    "# # Reading the content to a text file\n",
    "# with open('master_20190401.txt', 'rb') as f:\n",
    "#     byte_data = f.read()\n",
    "\n",
    "# # Decode byte data\n",
    "# data = byte_data.decode('utf-8').split('  ')\n",
    "\n",
    "# # find the starting index\n",
    "# for index, item in enumerate(data):\n",
    "\n",
    "#     if 'ftp://ftp.sec.gov/edgar/' in item:\n",
    "#         start_ind = index\n",
    "\n",
    "# # create a new list that removes unecessary items\n",
    "# data_format = data[start_ind + 1:]\n",
    "\n",
    "# master_data = []\n",
    "\n",
    "# #loop through the data list\n",
    "# for index, item in enumerate(data_format):\n",
    "\n",
    "#     if index == 0:\n",
    "#         clean_item_data = item.replace('\\n', '|').split('|')\n",
    "#         clean_item_data = clean_item_data[8:]\n",
    "#     else:\n",
    "#         clean_item_data = item.replace('\\n', '|').split('|')\n",
    "\n",
    "#     for index, row in enumerate(clean_item_data):\n",
    "\n",
    "#         #when you find the .txt file\n",
    "#         if '.txt' in row:\n",
    "\n",
    "#             mini_list = clean_item_data[(index - 4): index + 1]\n",
    "\n",
    "#             if len(mini_list) != 0:\n",
    "#                 mini_list[4] = \"https://www.sec.gov/Archives/\" + mini_list[4]\n",
    "#                 master_data.append(mini_list)\n",
    "\n",
    "# #master_data[:3]\n",
    "\n",
    "# # Loop through the master data set\n",
    "# for index, document in enumerate(master_data):\n",
    "\n",
    "#     # create a dictionary\n",
    "#     document_dict = {}\n",
    "#     document_dict['cik_number'] = document[0]\n",
    "#     document_dict['company_name'] = document[1]\n",
    "#     document_dict['form_id'] = document[2]\n",
    "#     document_dict['date'] = document[3]\n",
    "#     document_dict['file_url'] = document[4].replace('-','') # This will remove the '-' from the url\n",
    "\n",
    "#     master_data[index] = document_dict\n",
    "\n",
    "# urls = []\n",
    "# # Collect url of interest from the master dataset\n",
    "# for document_dict in master_data:\n",
    "#     if document_dict['form_id'] == 'DEF 14A': #Note: form id is case sentitive\n",
    "#         #print(document_dict['company_name'])\n",
    "#         target_url = document_dict['file_url']\n",
    "#         target_url = target_url[:-4] + '/index.json'\n",
    "#         #print(target_url)\n",
    "#         urls.append(target_url)\n",
    "\n",
    "# #print(urls)\n",
    "\n",
    "# # redefine base url \n",
    "# base_url = r\"https://www.sec.gov\"\n",
    "# Def14aLinks = []\n",
    "# #loop through urls to get only the DEF 14A url\n",
    "# for url in urls:\n",
    "#     content = requests.get(url).json()\n",
    "\n",
    "#     for file in content['directory']['item']:\n",
    "#         if \"14a\" in file['name'] and file['name'].endswith(('htm', 'html')):\n",
    "#             htm_summary = base_url + content['directory']['name'] + '/' + file['name']\n",
    "\n",
    "# #             print('-'*100)\n",
    "# #             print('File name:' + file['name'])\n",
    "# #             print('File path:' + htm_summary)\n",
    "# #             print(htm_summary)\n",
    "#             Def14aLinks.append(htm_summary)\n",
    "\n",
    "# print(Def14aLinks[0:5])          \n",
    "# data_links = pd.DataFrame(Def14aLinks)\n",
    "# data_links.to_csv('CSV FILE PATH HERE', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing and creating the list of dataframes (called \"tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the list of dataframes, called \"tables\"\n",
    "\n",
    "# Import packages\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "with open('YOUR_DATA_FILE.csv') as fp:\n",
    "    def14aLinks = [filename.strip() for filename in fp.readlines()]\n",
    "    def14aLinks = def14aLinks[1:]\n",
    "    \n",
    "tables = []\n",
    "for link in def14aLinks:\n",
    "    \n",
    "    content = requests.get(link)\n",
    "    soup = BeautifulSoup(content.text, 'lxml')\n",
    "\n",
    "    try:\n",
    "        search_result = pd.read_html(link, match=\"Beneficial\", header=0)\n",
    "        for df in search_result:\n",
    "            df = df.astype('str')\n",
    "            for column in df.columns:\n",
    "                if df[column].str.lower().str.contains(\"%\").any():\n",
    "                    tables.append(df)\n",
    "                    break\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Find and add company name\n",
    "    try:\n",
    "        company = soup.select_one(\"p:contains(Name)\").find_previous('p').text.strip()\n",
    "        df['Company Name'] = company\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    # 5. Adding Date\n",
    "    try:\n",
    "        date = soup.find('p', text = re.compile(r\"(?:January|February|March|April|May|June|July|August|September|October|November|December)\")).text.strip()\n",
    "        df['Filing Date'] = date\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    #print(df)\n",
    "    \n",
    "    #NOTE: THIS IS THE LONGEST PART OF THE CODE. MAY TAKE UP TO 4 HOURS, DEPENDING ON NUMBER OF ITERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving each dataframe in \"tables\" in an individual csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save each table to individual csv file\n",
    "def individual_saver(t, year):\n",
    "    #index count\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            temp_var = t[i]\n",
    "            temp_var.to_csv('YOUR_PATH_HERE/Raw_csv/{}/sample{}.csv'.format(year, i))\n",
    "            i += 1\n",
    "        except IndexError:\n",
    "            break\n",
    "\n",
    "individual_saver(tables, 1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tidying every dataframes in \"tables\" and appending result to initially created csv (See \"Def 14A Final _ First CSV\" notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying and appending every subsequent dataframe to initial CSV\n",
    "def tidy_saver(t):\n",
    "    #index count\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            temp_var = t[i]\n",
    "            temp_var = pd.DataFrame(temp_var)\n",
    "            temp_var.columns = temp_var.iloc[0]\n",
    "            temp_var = temp_var.drop(temp_var.index[0])\n",
    "            try:\n",
    "                temp_var = temp_var.drop(['nan'], axis = 1)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            column_names = ['Beneficial_Owner', 'share','Shares voting power', 'percent','Percent shares', 'Company name', 'Date filed']\n",
    "            if len(temp_var.columns) == 7:\n",
    "                temp_var.columns = column_names\n",
    "                #temp_var = temp_var.drop(['share','percent'], axis = 1) #dropping these may not be ideal\n",
    "                # as some values could land there. So, just clean them manually later on\n",
    "            else:\n",
    "                pass\n",
    "            temp_var.to_csv('YOUR_DATA_FILE.csv', mode='a', header=False)\n",
    "            # Note: while header = False, the index counting restarts at every new table appended.\n",
    "            # So, use index counting to identify where to start editing a table\n",
    "            i += 1\n",
    "        except IndexError:\n",
    "            break\n",
    "\n",
    "tidy_saver(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
